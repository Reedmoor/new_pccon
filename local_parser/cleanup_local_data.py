#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö JSON —Ñ–∞–π–ª–æ–≤
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞–ø–∫—É local_data –∏ —É–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥—É–±–ª–∏
"""

import os
import json
import glob
import logging
from datetime import datetime
from typing import Dict, List, Set
from pathlib import Path
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('cleanup_local_data')

class LocalDataCleaner:
    def __init__(self, data_dir="../local_data"):
        """
        –û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        
        Args:
            data_dir: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        self.data_dir = Path(data_dir).resolve()
        
        if not self.data_dir.exists():
            raise FileNotFoundError(f"Data directory not found: {self.data_dir}")
        
        logger.info(f"Local Data Cleaner initialized")
        logger.info(f"Data directory: {self.data_dir}")
    
    def analyze_files(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ local_data"""
        logger.info("üîç Analyzing local data files...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: local_CATEGORY_TIMESTAMP.json
        pattern = r"local_([^_]+(?:_[^_]+)*)_(\d{8}_\d{6})\.(?:json|meta\.json)$"
        
        files_by_category = {}
        meta_files = {}
        json_files = {}
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
        for file_path in self.data_dir.glob("*.json"):
            match = re.match(pattern, file_path.name)
            if match:
                category = match.group(1)
                timestamp = match.group(2)
                
                if file_path.name.endswith('.meta.json'):
                    if category not in meta_files:
                        meta_files[category] = []
                    meta_files[category].append({
                        'path': file_path,
                        'timestamp': timestamp,
                        'size': file_path.stat().st_size,
                        'modified': datetime.fromtimestamp(file_path.stat().st_mtime)
                    })
                else:  # .json —Ñ–∞–π–ª—ã
                    if category not in json_files:
                        json_files[category] = []
                    json_files[category].append({
                        'path': file_path,
                        'timestamp': timestamp,
                        'size': file_path.stat().st_size,
                        'modified': datetime.fromtimestamp(file_path.stat().st_mtime)
                    })
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        all_categories = set(list(meta_files.keys()) + list(json_files.keys()))
        
        for category in all_categories:
            files_by_category[category] = {
                'json_files': json_files.get(category, []),
                'meta_files': meta_files.get(category, []),
                'total_files': len(json_files.get(category, [])) + len(meta_files.get(category, []))
            }
        
        return files_by_category
    
    def get_duplicates_analysis(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥—É–±–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        files_by_category = self.analyze_files()
        
        duplicates = {}
        unique_categories = {}
        total_files = 0
        total_duplicates = 0
        total_size = 0
        duplicate_size = 0
        
        for category, files_info in files_by_category.items():
            json_files = files_info['json_files']
            meta_files = files_info['meta_files']
            
            total_files += len(json_files) + len(meta_files)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥—É–±–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if len(json_files) > 1 or len(meta_files) > 1:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
                json_files.sort(key=lambda x: x['timestamp'], reverse=True)
                meta_files.sort(key=lambda x: x['timestamp'], reverse=True)
                
                duplicates[category] = {
                    'json_files': {
                        'latest': json_files[0] if json_files else None,
                        'duplicates': json_files[1:] if len(json_files) > 1 else []
                    },
                    'meta_files': {
                        'latest': meta_files[0] if meta_files else None,
                        'duplicates': meta_files[1:] if len(meta_files) > 1 else []
                    }
                }
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                json_dups = len(json_files) - 1 if len(json_files) > 1 else 0
                meta_dups = len(meta_files) - 1 if len(meta_files) > 1 else 0
                total_duplicates += json_dups + meta_dups
                
                # –†–∞–∑–º–µ—Ä –¥—É–±–ª–µ–π
                for dup in json_files[1:]:
                    duplicate_size += dup['size']
                for dup in meta_files[1:]:
                    duplicate_size += dup['size']
            
            else:
                unique_categories[category] = files_info
            
            # –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä
            for file_info in json_files + meta_files:
                total_size += file_info['size']
        
        return {
            'duplicates': duplicates,
            'unique_categories': unique_categories,
            'stats': {
                'total_files': total_files,
                'total_duplicates': total_duplicates,
                'total_size_mb': total_size / 1024 / 1024,
                'duplicate_size_mb': duplicate_size / 1024 / 1024,
                'categories_with_duplicates': len(duplicates),
                'unique_categories': len(unique_categories)
            }
        }
    
    def show_analysis(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–µ–π"""
        analysis = self.get_duplicates_analysis()
        duplicates = analysis['duplicates']
        unique_categories = analysis['unique_categories']
        stats = analysis['stats']
        
        logger.info("="*70)
        logger.info("üìä –ê–ù–ê–õ–ò–ó –õ–û–ö–ê–õ–¨–ù–´–• –î–£–ë–õ–ò–†–£–Æ–©–ò–• –§–ê–ô–õ–û–í")
        logger.info("="*70)
        
        if duplicates:
            logger.info(f"‚ùå –ù–∞–π–¥–µ–Ω–æ {stats['categories_with_duplicates']} –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –¥—É–±–ª—è–º–∏:")
            
            for category, files_info in duplicates.items():
                json_info = files_info['json_files']
                meta_info = files_info['meta_files']
                
                logger.info(f"\nüìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
                
                # JSON —Ñ–∞–π–ª—ã
                if json_info['latest']:
                    logger.info(f"   üìÑ JSON —Ñ–∞–π–ª—ã:")
                    latest = json_info['latest']
                    logger.info(f"      ‚úÖ –û—Å—Ç–∞–≤–∏—Ç—å: {latest['path'].name}")
                    logger.info(f"         –†–∞–∑–º–µ—Ä: {latest['size'] / 1024:.1f} KB")
                    logger.info(f"         –í—Ä–µ–º—è: {latest['timestamp']}")
                    
                    if json_info['duplicates']:
                        logger.info(f"      ‚ùå –£–¥–∞–ª–∏—Ç—å {len(json_info['duplicates'])} –¥—É–±–ª–µ–π:")
                        for dup in json_info['duplicates']:
                            logger.info(f"         ‚Ä¢ {dup['path'].name} ({dup['size'] / 1024:.1f} KB)")
                
                # Meta —Ñ–∞–π–ª—ã
                if meta_info['latest']:
                    logger.info(f"   üìù Meta —Ñ–∞–π–ª—ã:")
                    latest = meta_info['latest']
                    logger.info(f"      ‚úÖ –û—Å—Ç–∞–≤–∏—Ç—å: {latest['path'].name}")
                    
                    if meta_info['duplicates']:
                        logger.info(f"      ‚ùå –£–¥–∞–ª–∏—Ç—å {len(meta_info['duplicates'])} –¥—É–±–ª–µ–π:")
                        for dup in meta_info['duplicates']:
                            logger.info(f"         ‚Ä¢ {dup['path'].name}")
        
        if unique_categories:
            logger.info(f"\n‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ({len(unique_categories)}):")
            for category, files_info in unique_categories.items():
                json_count = len(files_info['json_files'])
                meta_count = len(files_info['meta_files'])
                logger.info(f"   ‚Ä¢ {category} - {json_count} JSON + {meta_count} meta")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {stats['total_files']}")
        logger.info(f"   –î—É–±–ª–µ–π –∫ —É–¥–∞–ª–µ–Ω–∏—é: {stats['total_duplicates']}")
        logger.info(f"   –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {stats['total_size_mb']:.1f} MB")
        logger.info(f"   –†–∞–∑–º–µ—Ä –¥—É–±–ª–µ–π: {stats['duplicate_size_mb']:.1f} MB")
        logger.info(f"   –û—Å–≤–æ–±–æ–¥–∏—Ç—Å—è –º–µ—Å—Ç–∞: {stats['duplicate_size_mb']:.1f} MB")
        logger.info(f"   –û—Å—Ç–∞–Ω–µ—Ç—Å—è —Ñ–∞–π–ª–æ–≤: {stats['total_files'] - stats['total_duplicates']}")
        
        if not duplicates:
            logger.info("‚úÖ –î—É–±–ª–∏—Ä—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
    
    def simulate_cleanup(self):
        """–°–∏–º—É–ª—è—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á—Ç–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ)"""
        analysis = self.get_duplicates_analysis()
        duplicates = analysis['duplicates']
        stats = analysis['stats']
        
        if not duplicates:
            logger.info("‚úÖ –ù–µ—Ç –¥—É–±–ª–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            return
        
        logger.info("üîç –°–ò–ú–£–õ–Ø–¶–ò–Ø –û–ß–ò–°–¢–ö–ò (—Ñ–∞–π–ª—ã –ù–ï —É–¥–∞–ª—è—é—Ç—Å—è):")
        logger.info("="*50)
        
        files_to_delete = []
        
        for category, files_info in duplicates.items():
            json_duplicates = files_info['json_files']['duplicates']
            meta_duplicates = files_info['meta_files']['duplicates']
            
            logger.info(f"\nüìÇ {category}:")
            
            for dup in json_duplicates:
                logger.info(f"üóëÔ∏è  JSON: {dup['path'].name} ({dup['size'] / 1024:.1f} KB)")
                files_to_delete.append(str(dup['path']))
            
            for dup in meta_duplicates:
                logger.info(f"üóëÔ∏è  Meta: {dup['path'].name}")
                files_to_delete.append(str(dup['path']))
        
        logger.info(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ –°–ò–ú–£–õ–Ø–¶–ò–ò:")
        logger.info(f"   –§–∞–π–ª–æ–≤ –∫ —É–¥–∞–ª–µ–Ω–∏—é: {len(files_to_delete)}")
        logger.info(f"   –û—Å–≤–æ–±–æ–¥–∏—Ç—Å—è –º–µ—Å—Ç–∞: {stats['duplicate_size_mb']:.1f} MB")
        logger.info(f"   –û—Å—Ç–∞–Ω–µ—Ç—Å—è —Ñ–∞–π–ª–æ–≤: {stats['total_files'] - len(files_to_delete)}")
    
    def cleanup_duplicates(self, dry_run=True):
        """–†–µ–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–µ–π"""
        analysis = self.get_duplicates_analysis()
        duplicates = analysis['duplicates']
        
        if not duplicates:
            logger.info("‚úÖ –ù–µ—Ç –¥—É–±–ª–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            return 0
        
        if dry_run:
            logger.info("üîç DRY RUN - —Ñ–∞–π–ª—ã –ù–ï –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã")
        else:
            logger.info("‚ö†Ô∏è  –†–ï–ê–õ–¨–ù–ê–Ø –û–ß–ò–°–¢–ö–ê - —Ñ–∞–π–ª—ã –ë–£–î–£–¢ —É–¥–∞–ª–µ–Ω—ã!")
        
        deleted_count = 0
        
        for category, files_info in duplicates.items():
            json_duplicates = files_info['json_files']['duplicates']
            meta_duplicates = files_info['meta_files']['duplicates']
            
            # –£–¥–∞–ª—è–µ–º JSON –¥—É–±–ª–∏
            for dup in json_duplicates:
                try:
                    if not dry_run:
                        dup['path'].unlink()
                    logger.info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω: {dup['path'].name}")
                    deleted_count += 1
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {dup['path'].name}: {e}")
            
            # –£–¥–∞–ª—è–µ–º meta –¥—É–±–ª–∏
            for dup in meta_duplicates:
                try:
                    if not dry_run:
                        dup['path'].unlink()
                    logger.info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω: {dup['path'].name}")
                    deleted_count += 1
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {dup['path'].name}: {e}")
        
        return deleted_count


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Cleanup duplicate local data files')
    parser.add_argument('--data-dir', type=str, default='../local_data',
                       help='Local data directory (default: ../local_data)')
    parser.add_argument('--analyze', action='store_true',
                       help='Analyze duplicate files')
    parser.add_argument('--simulate', action='store_true',
                       help='Simulate cleanup (show what would be deleted)')
    parser.add_argument('--cleanup', action='store_true',
                       help='Perform real cleanup (DELETE files)')
    parser.add_argument('--dry-run', action='store_true', default=True,
                       help='Dry run mode (default: true)')
    
    args = parser.parse_args()
    
    try:
        cleaner = LocalDataCleaner(data_dir=args.data_dir)
        
        if args.analyze:
            cleaner.show_analysis()
        elif args.simulate:
            cleaner.simulate_cleanup()
        elif args.cleanup:
            if not args.dry_run:
                # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è
                response = input("‚ö†Ô∏è  –í—ã —É–≤–µ—Ä–µ–Ω—ã —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ —Ñ–∞–π–ª—ã? (y/N): ")
                if response.lower() != 'y':
                    logger.info("–û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
                    return 0
            
            deleted = cleaner.cleanup_duplicates(dry_run=args.dry_run)
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {deleted}")
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑
            cleaner.show_analysis()
            logger.info("\nüí° –ö–æ–º–∞–Ω–¥—ã:")
            logger.info("   --simulate  - —Å–∏–º—É–ª—è—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏")
            logger.info("   --cleanup   - —Ä–µ–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ (—Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º)")
        
        return 0
    
    except KeyboardInterrupt:
        logger.info("Operation interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"Error: {e}")
        return 1


if __name__ == '__main__':
    exit(main()) 