#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö JSON —Ñ–∞–π–ª–æ–≤ –Ω–∞ Docker —Å–µ—Ä–≤–µ—Ä–µ
–û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ–∞–π–ª—ã, —É–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥—É–±–ª–∏
"""

import requests
import json
import logging
from datetime import datetime
from typing import List, Dict

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('cleanup_duplicates')

class DockerServerCleaner:
    def __init__(self, docker_url="http://127.0.0.1:5000"):
        """
        –û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –Ω–∞ Docker —Å–µ—Ä–≤–µ—Ä–µ
        
        Args:
            docker_url: URL Docker —Å–µ—Ä–≤–µ—Ä–∞
        """
        self.docker_url = docker_url.rstrip('/')
        self.session = requests.Session()
        
        logger.info(f"Docker Server Cleaner initialized")
        logger.info(f"Docker server: {self.docker_url}")
    
    def get_upload_files(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            response = self.session.get(f"{self.docker_url}/api/parser-status", timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                uploads = data.get('recent_uploads', [])
                logger.info(f"Found {len(uploads)} upload files")
                return uploads
            else:
                logger.error(f"Failed to get uploads: status {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"Error getting upload files: {e}")
            return []
    
    def analyze_duplicates(self, uploads: List[Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥—É–±–ª–∏ —Ñ–∞–π–ª–æ–≤"""
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–≤–∞—Ä–æ–≤
        groups = {}
        for upload in uploads:
            count = upload.get('product_count', 0)
            filename = upload.get('filename', '')
            
            if count not in groups:
                groups[count] = []
            groups[count].append(upload)
        
        # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏ (—Ñ–∞–π–ª—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–æ–≤–∞—Ä–æ–≤)
        duplicates = {}
        unique_files = {}
        
        for count, files in groups.items():
            if len(files) > 1:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
                files.sort(key=lambda x: x.get('upload_time', ''), reverse=True)
                duplicates[count] = {
                    'latest': files[0],
                    'duplicates': files[1:],
                    'total_duplicates': len(files) - 1
                }
            else:
                unique_files[count] = files[0]
        
        return {
            'duplicates': duplicates,
            'unique_files': unique_files,
            'total_files': len(uploads)
        }
    
    def show_duplicate_analysis(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        uploads = self.get_upload_files()
        if not uploads:
            logger.info("No upload files found")
            return
        
        analysis = self.analyze_duplicates(uploads)
        duplicates = analysis['duplicates']
        unique_files = analysis['unique_files']
        
        logger.info("="*50)
        logger.info("üìä –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–†–£–Æ–©–ò–• –§–ê–ô–õ–û–í")
        logger.info("="*50)
        
        if duplicates:
            logger.info(f"‚ùå –ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –≥—Ä—É–ø–ø –¥—É–±–ª–µ–π:")
            
            total_duplicate_files = 0
            for count, group in duplicates.items():
                latest = group['latest']
                dups = group['duplicates']
                total_duplicate_files += len(dups)
                
                logger.info(f"\nüî¢ –§–∞–π–ª—ã —Å {count:,} —Ç–æ–≤–∞—Ä–∞–º–∏:")
                logger.info(f"   ‚úÖ –û—Å—Ç–∞–≤–∏—Ç—å: {latest['filename']}")
                logger.info(f"      –í—Ä–µ–º—è: {latest['upload_time']}")
                
                logger.info(f"   ‚ùå –£–¥–∞–ª–∏—Ç—å {len(dups)} –¥—É–±–ª–µ–π:")
                for dup in dups:
                    logger.info(f"      ‚Ä¢ {dup['filename']} ({dup['upload_time']})")
            
            logger.info(f"\nüìã –ò–¢–û–ì–û:")
            logger.info(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {analysis['total_files']}")
            logger.info(f"   –î—É–±–ª–µ–π –∫ —É–¥–∞–ª–µ–Ω–∏—é: {total_duplicate_files}")
            logger.info(f"   –û—Å—Ç–∞–Ω–µ—Ç—Å—è —Ñ–∞–π–ª–æ–≤: {analysis['total_files'] - total_duplicate_files}")
        
        if unique_files:
            logger.info(f"\n‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã ({len(unique_files)}):")
            for count, file_info in unique_files.items():
                logger.info(f"   ‚Ä¢ {file_info['filename']} - {count:,} —Ç–æ–≤–∞—Ä–æ–≤")
        
        if not duplicates:
            logger.info("‚úÖ –î—É–±–ª–∏—Ä—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
    
    def cleanup_duplicates_simulation(self):
        """–°–∏–º—É–ª—è—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ –¥—É–±–ª–µ–π (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á—Ç–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ)"""
        uploads = self.get_upload_files()
        if not uploads:
            return
        
        analysis = self.analyze_duplicates(uploads)
        duplicates = analysis['duplicates']
        
        if not duplicates:
            logger.info("‚úÖ –ù–µ—Ç –¥—É–±–ª–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            return
        
        logger.info("üîç –°–ò–ú–£–õ–Ø–¶–ò–Ø –û–ß–ò–°–¢–ö–ò (—Ñ–∞–π–ª—ã –ù–ï —É–¥–∞–ª—è—é—Ç—Å—è):")
        logger.info("="*50)
        
        total_to_delete = 0
        saved_space = 0
        
        for count, group in duplicates.items():
            dups = group['duplicates']
            
            for dup in dups:
                file_size = dup.get('file_size', 0)
                logger.info(f"üóëÔ∏è  –ë—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω: {dup['filename']}")
                logger.info(f"     –†–∞–∑–º–µ—Ä: {file_size / 1024:.1f} KB")
                logger.info(f"     –í—Ä–µ–º—è: {dup['upload_time']}")
                total_to_delete += 1
                saved_space += file_size
        
        logger.info(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ –°–ò–ú–£–õ–Ø–¶–ò–ò:")
        logger.info(f"   –§–∞–π–ª–æ–≤ –∫ —É–¥–∞–ª–µ–Ω–∏—é: {total_to_delete}")
        logger.info(f"   –û—Å–≤–æ–±–æ–¥–∏—Ç—Å—è –º–µ—Å—Ç–∞: {saved_space / 1024 / 1024:.1f} MB")
        logger.info(f"   –û—Å—Ç–∞–Ω–µ—Ç—Å—è —Ñ–∞–π–ª–æ–≤: {analysis['total_files'] - total_to_delete}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Cleanup duplicate files on Docker server')
    parser.add_argument('--docker-url', type=str, default='http://127.0.0.1:5000',
                       help='Docker server URL (default: http://127.0.0.1:5000)')
    parser.add_argument('--analyze', action='store_true',
                       help='Analyze duplicate files')
    parser.add_argument('--simulate', action='store_true', 
                       help='Simulate cleanup (show what would be deleted)')
    
    args = parser.parse_args()
    
    cleaner = DockerServerCleaner(docker_url=args.docker_url)
    
    try:
        if args.analyze:
            cleaner.show_duplicate_analysis()
        elif args.simulate:
            cleaner.cleanup_duplicates_simulation()
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑
            cleaner.show_duplicate_analysis()
            logger.info("\nüí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --simulate –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏")
        
        return 0
    
    except KeyboardInterrupt:
        logger.info("Operation interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return 1


if __name__ == '__main__':
    exit(main()) 